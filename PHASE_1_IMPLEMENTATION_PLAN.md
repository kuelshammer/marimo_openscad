# üß™ Phase 1 Implementation Plan: Test Coverage Foundation

**Phase Start Date:** 13. Juni 2025  
**Duration:** 4 Tage  
**Current Status:** ‚úÖ 80% IMPLEMENTED - Ready for Execution
**Goal:** Echte Probleme sichtbar machen, False Confidence eliminieren  
**Expected Result:** ‚ùå Tests werden FEHLSCHLAGEN (erw√ºnscht!)

## üéØ **Phase 1 Strategic Objectives**

### **Primary Goal: Eliminate Mock-Based False Confidence**
Aktuelle Situation: 90% Mock-basierte Tests geben false positives
- Ziel: 0% false positive confidence 
- Methode: Browser-basierte E2E Tests ohne Mocks
- Result: Echte Probleme werden sichtbar und dokumentiert

### **Secondary Goal: Foundation f√ºr Fix Validation**
- Infrastruktur f√ºr Phases 2-4 Fix-Validierung
- Real User Journey Testing
- Performance Baseline Establishment

---

## üìã **Step 1.1: End-to-End Test Infrastructure** 
**Duration:** 1.5 Tage  
**Priority:** ‚≠ê CRITICAL  
**Status:** ‚úÖ COMPLETED - Tests already implemented

### **Step 1.1.1: Playwright Browser Testing Setup**

#### **Dependencies Installation** ‚úÖ READY
```bash
# Add Playwright f√ºr browser-based testing
uv add --group dev playwright pytest-playwright

# Install browsers
npx playwright install chromium firefox webkit

# Add browser testing dev dependencies
uv add --group dev pytest-html pytest-xvfb
```

#### **Playwright Configuration** ‚úÖ IMPLEMENTED
```python
# tests/playwright_config.py - BEREITS VORHANDEN
import pytest
from playwright.sync_api import sync_playwright

@pytest.fixture(scope="session")
def browser_context():
    """Real browser context for anywidget testing"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            viewport={'width': 1280, 'height': 720},
            permissions=['camera', 'microphone']  # For potential WASM needs
        )
        yield context
        context.close()
        browser.close()

@pytest.fixture
def test_page(browser_context):
    """Clean page for each test"""
    page = browser_context.new_page()
    yield page
    page.close()
```

#### **HTML Test Environment** ‚úÖ IMPLEMENTED
```html
<!-- tests/test_files/anywidget_test_environment.html - BEREITS VORHANDEN -->
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>anywidget Real Browser Test</title>
    <script type="module">
        // Load real anywidget environment
        import { createWidget } from '../src/js/widget.js';
        
        window.testAnyWidget = async function(scadCode) {
            const container = document.getElementById('widget-container');
            try {
                const widget = await createWidget({
                    scad_code: scadCode,
                    renderer_type: 'auto'
                });
                container.appendChild(widget);
                return { success: true, widget };
            } catch (error) {
                return { success: false, error: error.message };
            }
        };
    </script>
</head>
<body>
    <div id="widget-container"></div>
    <div id="test-results"></div>
</body>
</html>
```

---

### **Step 1.1.2: Real anywidget Execution Tests** ‚úÖ IMPLEMENTED

#### **JavaScript Module Loading Test** ‚úÖ IMPLEMENTED
```python
# tests/test_e2e_anywidget_real.py - BEREITS VORHANDEN
import pytest
from playwright.sync_api import Page
import json

class TestRealAnyWidgetExecution:
    """Test echte anywidget JavaScript-Ausf√ºhrung ohne Mocks"""
    
    def test_anywidget_module_loading_failure(self, test_page: Page):
        """
        EXPECTED TO FAIL: Test dynamic import limitations
        
        Dies ist ein NEGATIVER Test - er soll die Import-Probleme dokumentieren
        """
        test_page.goto(f"file://{Path(__file__).parent}/test_anywidget_environment.html")
        
        # Versuche anywidget mit echten imports zu laden
        result = test_page.evaluate("""
            async () => {
                try {
                    // Dies sollte FEHLSCHLAGEN wegen relative imports
                    const result = await window.testAnyWidget('cube([1,1,1]);');
                    return result;
                } catch (error) {
                    return { 
                        success: false, 
                        error: error.message,
                        errorType: 'ImportError'
                    };
                }
            }
        """)
        
        # ERWARTUNG: Test schl√§gt fehl, aber dokumentiert das Problem
        assert result['success'] is False
        assert 'import' in result['error'].lower() or 'module' in result['error'].lower()
        
        # Log f√ºr Problem-Dokumentation
        print(f"üîç DOCUMENTED PROBLEM: {result['error']}")
    
    def test_wasm_loading_attempt_real(self, test_page: Page):
        """
        EXPECTED TO FAIL: Test WASM module loading ohne Browser-Mocks
        """
        test_page.goto(f"file://{Path(__file__).parent}/test_anywidget_environment.html")
        
        # Versuche WASM-Module zu laden
        wasm_result = test_page.evaluate("""
            async () => {
                try {
                    // Versuche WASM-Dateien vom lokalen Pfad zu laden
                    const wasmResponse = await fetch('/static/wasm/openscad.wasm');
                    return {
                        success: wasmResponse.ok,
                        status: wasmResponse.status,
                        error: wasmResponse.ok ? null : 'WASM file not accessible'
                    };
                } catch (error) {
                    return {
                        success: false,
                        error: error.message,
                        errorType: 'NetworkError'
                    };
                }
            }
        """)
        
        # ERWARTUNG: WASM loading fehlschl√§gt
        assert wasm_result['success'] is False
        print(f"üîç DOCUMENTED WASM PROBLEM: {wasm_result['error']}")
    
    def test_browser_console_errors_capture(self, test_page: Page):
        """Capture echte Browser console errors"""
        console_messages = []
        test_page.on("console", lambda msg: console_messages.append({
            'type': msg.type,
            'text': msg.text,
            'location': msg.location
        }))
        
        test_page.goto(f"file://{Path(__file__).parent}/test_anywidget_environment.html")
        
        # Trigger widget loading attempt
        test_page.evaluate("window.testAnyWidget('cube([2,2,2]);')")
        
        # Dokumentiere alle console errors
        errors = [msg for msg in console_messages if msg['type'] == 'error']
        assert len(errors) > 0, "Expected console errors from import failures"
        
        for error in errors:
            print(f"üîç BROWSER ERROR: {error['text']} at {error['location']}")
```

---

### **Step 1.1.3: Real Marimo Notebook Execution Tests** ‚ö†Ô∏è PARTIALLY IMPLEMENTED

#### **Marimo Integration Test** ‚ö†Ô∏è TODO
```python
# tests/test_e2e_marimo_real.py - NOCH ZU IMPLEMENTIEREN
import pytest
import subprocess
import tempfile
from pathlib import Path
import time

class TestRealMarimoExecution:
    """Test echte Marimo notebook execution ohne Mocks"""
    
    def test_marimo_variable_conflicts_real(self):
        """
        EXPECTED TO REVEAL CONFLICTS: Test variable conflicts in multi-cell execution
        """
        # Erstelle tempor√§re Marimo notebook mit problematic variables
        notebook_content = '''
import marimo

__generated_with = "0.13.15"
app = marimo.App(width="medium")

@app.cell
def test_cell_1():
    from src.marimo_openscad.viewer import openscad_viewer
    from solid2 import cube
    
    # Problematic variable name pattern
    viewer_test = openscad_viewer(cube([1,1,1]), renderer_type="auto")
    return viewer_test,

@app.cell  
def test_cell_2():
    from src.marimo_openscad.viewer import openscad_viewer
    from solid2 import sphere
    
    # Same variable name pattern - should conflict
    viewer_test = openscad_viewer(sphere(r=1), renderer_type="auto")
    return viewer_test,

@app.cell
def validation_cell(viewer_test):
    # This cell depends on viewer_test - which one will it get?
    print(f"Viewer type: {type(viewer_test)}")
    print(f"Model data: {getattr(viewer_test, 'stl_data', 'NO STL DATA')[:100]}")
    return

if __name__ == "__main__":
    app.run()
'''
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(notebook_content)
            notebook_path = f.name
        
        try:
            # Execute notebook und capture output
            result = subprocess.run([
                'marimo', 'run', notebook_path, '--headless'
            ], capture_output=True, text=True, timeout=30)
            
            # Dokumentiere Ergebnisse (expected conflicts/errors)
            if result.returncode != 0:
                print(f"üîç MARIMO EXECUTION PROBLEMS:")
                print(f"STDERR: {result.stderr}")
                print(f"STDOUT: {result.stdout}")
            
            # Check f√ºr variable conflict indicators
            output = result.stdout + result.stderr
            conflict_indicators = [
                'redefinition', 'conflict', 'undefined', 'NameError'
            ]
            
            found_conflicts = [ind for ind in conflict_indicators if ind in output.lower()]
            if found_conflicts:
                print(f"üîç DETECTED CONFLICTS: {found_conflicts}")
            
            # Document execution inconsistency
            assert len(found_conflicts) > 0 or result.returncode != 0, \
                "Expected variable conflicts or execution issues"
                
        finally:
            Path(notebook_path).unlink()
    
    def test_marimo_multi_execution_consistency(self):
        """Test ob multiple executions konsistente Ergebnisse liefern"""
        
        simple_notebook = '''
import marimo
app = marimo.App()

@app.cell
def _():
    from src.marimo_openscad.viewer import openscad_viewer
    from solid2 import cube
    model = cube([2,2,2])
    viewer = openscad_viewer(model, renderer_type="auto")
    print(f"Execution result: {hash(str(viewer.stl_data))}")
    return viewer,

if __name__ == "__main__":
    app.run()
'''
        
        results = []
        for i in range(3):
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(simple_notebook)
                
                result = subprocess.run([
                    'marimo', 'run', f.name, '--headless'
                ], capture_output=True, text=True, timeout=20)
                
                results.append({
                    'returncode': result.returncode,
                    'stdout': result.stdout,
                    'stderr': result.stderr
                })
                
                Path(f.name).unlink()
        
        # Check f√ºr Konsistenz
        return_codes = [r['returncode'] for r in results]
        stdout_hashes = [hash(r['stdout']) for r in results]
        
        print(f"üîç EXECUTION CONSISTENCY:")
        print(f"Return codes: {return_codes}")
        print(f"Output consistency: {len(set(stdout_hashes))} unique outputs")
        
        # Document inconsistency if present
        if len(set(return_codes)) > 1 or len(set(stdout_hashes)) > 1:
            print("üîç INCONSISTENT EXECUTION DETECTED")
            for i, result in enumerate(results):
                print(f"Run {i+1}: RC={result['returncode']}, OUT_HASH={hash(result['stdout'])}")
```

---

### **Step 1.1.4: CI Integration & Validation**

#### **GitHub Actions Browser Testing**
```yaml
# .github/workflows/phase1-e2e-tests.yml - NEU
name: Phase 1 E2E Tests (Expected Failures)

on: [push, pull_request]

jobs:
  e2e-testing:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup uv
      uses: astral-sh/setup-uv@v2
      
    - name: Install dependencies
      run: |
        uv sync --group dev
        uv add --group dev playwright pytest-playwright pytest-html
        
    - name: Install Playwright browsers
      run: npx playwright install chromium
      
    - name: Install Marimo
      run: uv add marimo
      
    - name: Run Phase 1 E2E Tests (Expected Failures)
      run: |
        # Erwarte test failures - das ist der Punkt!
        uv run pytest tests/test_e2e_*.py -v --html=phase1-test-report.html || true
        
    - name: Upload Test Results
      uses: actions/upload-artifact@v4
      with:
        name: phase1-test-results
        path: |
          phase1-test-report.html
          test-results/
          
    - name: Document Test Failures (Expected)
      run: |
        echo "## Phase 1 Test Results (Failures Expected)" >> $GITHUB_STEP_SUMMARY
        echo "These test failures document the 4 critical issues:" >> $GITHUB_STEP_SUMMARY
        echo "- anywidget import limitations" >> $GITHUB_STEP_SUMMARY  
        echo "- WASM loading failures" >> $GITHUB_STEP_SUMMARY
        echo "- Marimo variable conflicts" >> $GITHUB_STEP_SUMMARY
        echo "- Mock-hidden integration problems" >> $GITHUB_STEP_SUMMARY
```

---

## üìã **Step 1.2: Critical Path Testing**
**Duration:** 1 Tag  
**Priority:** ‚≠ê HIGH

### **User Journey 1: WASM CSG Operations End-to-End**
```python
# tests/test_critical_user_journeys.py - NEU
class TestCriticalUserJourneys:
    
    def test_wasm_csg_operations_e2e_failure(self):
        """
        EXPECTED TO FAIL: User erstellt CSG ‚Üí erwartet echte Geometrie ‚Üí bekommt Placeholder
        """
        from src.marimo_openscad.viewer import openscad_viewer
        from solid2 import cube, sphere, union
        
        # Create CSG operation
        box = cube([3, 3, 3])
        ball = sphere(r=1.8).translate([1.5, 1.5, 1.5])
        csg_union = union()(box, ball)
        
        # User expects real STL geometry
        viewer = openscad_viewer(csg_union, renderer_type="wasm")
        
        # Check what we actually get
        stl_data = viewer.stl_data
        
        print(f"üîç STL DATA TYPE: {type(stl_data)}")
        print(f"üîç STL DATA PREVIEW: {stl_data[:100] if stl_data else 'EMPTY'}")
        
        # EXPECTED FAILURE: Should get placeholder instead of real STL
        if isinstance(stl_data, str) and "WASM_RENDER_REQUEST" in stl_data:
            print("üîç CONFIRMED PROBLEM: Got placeholder instead of real STL")
            assert True, "Problem correctly identified"
        elif not stl_data or stl_data == "":
            print("üîç CONFIRMED PROBLEM: No STL data generated")
            assert True, "Problem correctly identified"
        else:
            # If we somehow get real STL, validate it's actually correct
            assert b"facet" in stl_data.encode() if isinstance(stl_data, str) else stl_data
    
    def test_renderer_fallback_chain_real(self):
        """Test complete fallback chain without mocks"""
        from src.marimo_openscad.viewer import openscad_viewer
        from solid2 import cube
        
        test_model = cube([2, 2, 2])
        
        # Test fallback sequence
        fallback_results = {}
        
        for renderer_type in ["wasm", "local", "auto"]:
            try:
                viewer = openscad_viewer(test_model, renderer_type=renderer_type)
                fallback_results[renderer_type] = {
                    'success': True,
                    'stl_data_type': type(viewer.stl_data).__name__,
                    'stl_data_length': len(viewer.stl_data) if viewer.stl_data else 0,
                    'renderer_status': getattr(viewer, 'renderer_status', 'unknown')
                }
            except Exception as e:
                fallback_results[renderer_type] = {
                    'success': False,
                    'error': str(e)
                }
        
        print("üîç FALLBACK CHAIN RESULTS:")
        for renderer, result in fallback_results.items():
            print(f"  {renderer}: {result}")
        
        # Document fallback gaps
        failed_renderers = [r for r, res in fallback_results.items() if not res['success']]
        if failed_renderers:
            print(f"üîç FAILED RENDERERS: {failed_renderers}")
```

### **User Journey 2: Multi-Cell Variable Conflicts**
```python
def test_marimo_multi_cell_variable_isolation(self):
    """Test variable isolation zwischen verschiedenen Zellen"""
    
    # Simuliere multi-cell environment
    cell_results = {}
    
    # Cell 1: cube viewer
    try:
        from src.marimo_openscad.viewer import openscad_viewer
        from solid2 import cube
        viewer_geo = openscad_viewer(cube([1,1,1]), renderer_type="auto")
        cell_results['cell_1'] = {
            'variable': 'viewer_geo',
            'type': type(viewer_geo).__name__,
            'id': id(viewer_geo)
        }
    except Exception as e:
        cell_results['cell_1'] = {'error': str(e)}
    
    # Cell 2: sphere viewer (same variable name pattern)
    try:
        from solid2 import sphere
        viewer_geo = openscad_viewer(sphere(r=1), renderer_type="auto") 
        cell_results['cell_2'] = {
            'variable': 'viewer_geo',
            'type': type(viewer_geo).__name__,
            'id': id(viewer_geo)
        }
    except Exception as e:
        cell_results['cell_2'] = {'error': str(e)}
    
    print("üîç VARIABLE CONFLICT ANALYSIS:")
    for cell, result in cell_results.items():
        print(f"  {cell}: {result}")
    
    # Check for variable overwrites
    if all('id' in res for res in cell_results.values()):
        if cell_results['cell_1']['id'] == cell_results['cell_2']['id']:
            print("üîç POTENTIAL VARIABLE CONFLICT: Same object ID")
        else:
            print("üîç VARIABLE ISOLATION: Different object IDs")
```

---

## üìã **Step 1.3: Performance & Load Testing**
**Duration:** 1 Tag  
**Priority:** MEDIUM

### **Performance Baseline Tests**
```python
# tests/test_performance_baseline.py - NEU
import time
import psutil
import pytest
from src.marimo_openscad.viewer import openscad_viewer
from solid2 import cube, sphere, union, difference

class TestPerformanceBaseline:
    """Establish performance baselines (will show poor performance initially)"""
    
    def test_wasm_vs_local_speed_baseline(self):
        """
        EXPECTED TO FAIL: WASM speed comparison (WASM nicht funktional)
        """
        test_model = union()(
            cube([5, 5, 5]),
            sphere(r=3).translate([2.5, 2.5, 2.5])
        )
        
        performance_results = {}
        
        # Test WASM performance (expected to fail)
        try:
            start_time = time.time()
            wasm_viewer = openscad_viewer(test_model, renderer_type="wasm")
            wasm_time = time.time() - start_time
            
            performance_results['wasm'] = {
                'time': wasm_time,
                'success': bool(wasm_viewer.stl_data),
                'stl_length': len(wasm_viewer.stl_data) if wasm_viewer.stl_data else 0
            }
        except Exception as e:
            performance_results['wasm'] = {
                'time': None,
                'success': False,
                'error': str(e)
            }
        
        # Test Local performance (should work)
        try:
            start_time = time.time()
            local_viewer = openscad_viewer(test_model, renderer_type="local")
            local_time = time.time() - start_time
            
            performance_results['local'] = {
                'time': local_time,
                'success': bool(local_viewer.stl_data),
                'stl_length': len(local_viewer.stl_data) if local_viewer.stl_data else 0
            }
        except Exception as e:
            performance_results['local'] = {
                'time': None,
                'success': False,
                'error': str(e)
            }
        
        print("üîç PERFORMANCE BASELINE:")
        for renderer, result in performance_results.items():
            print(f"  {renderer}: {result}")
        
        # Document performance gaps
        if not performance_results['wasm']['success']:
            print("üîç WASM PERFORMANCE: NOT MEASURABLE (renderer broken)")
        
        if performance_results['local']['success']:
            print(f"üîç LOCAL BASELINE: {performance_results['local']['time']:.2f}s")
    
    def test_memory_usage_constraints(self):
        """Test memory usage patterns (establish 2GB baseline)"""
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Create progressively larger models
        model_sizes = [1, 2, 5, 10]  # Cube sizes
        memory_usage = {'initial': initial_memory}
        
        for size in model_sizes:
            large_model = union()(*[
                cube([size, size, size]).translate([i*size, 0, 0]) 
                for i in range(3)
            ])
            
            try:
                viewer = openscad_viewer(large_model, renderer_type="auto")
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_usage[f'size_{size}'] = {
                    'memory_mb': current_memory,
                    'increase_mb': current_memory - initial_memory,
                    'stl_length': len(viewer.stl_data) if viewer.stl_data else 0
                }
            except Exception as e:
                memory_usage[f'size_{size}'] = {'error': str(e)}
        
        print("üîç MEMORY USAGE BASELINE:")
        for test, result in memory_usage.items():
            print(f"  {test}: {result}")
        
        # Check 2GB constraint compliance
        max_memory = max(
            r.get('memory_mb', 0) for r in memory_usage.values() 
            if isinstance(r, dict) and 'memory_mb' in r
        )
        
        if max_memory > 2048:  # 2GB
            print(f"üîç MEMORY CONSTRAINT VIOLATION: {max_memory:.1f}MB > 2GB")
        else:
            print(f"üîç MEMORY USAGE OK: Max {max_memory:.1f}MB < 2GB")
```

---

## ‚úÖ **Validation Gates & Success Criteria**

### **Gate 1.1: E2E Infrastructure Ready**
**Expected Results:**
- ‚úÖ Playwright successfully installed and configured
- ‚úÖ Browser-based tests executable
- ‚ùå Import errors DETECTED and DOCUMENTED (erw√ºnscht!)
- ‚ùå WASM loading failures DOCUMENTED (erw√ºnscht!)

**Validation Command:**
```bash
# Should show failures but infrastructure works
uv run pytest tests/test_e2e_anywidget_real.py -v
```

### **Gate 1.2: Critical Paths Failing Visibly**
**Expected Results:**
- ‚ùå CSG operations show placeholders instead of STL (problem visible!)
- ‚ùå Variable conflicts detected and documented
- ‚ùå Fallback chain has gaps

**Validation Command:**
```bash
# Should document all critical problems
uv run pytest tests/test_critical_user_journeys.py -v --tb=short
```

### **Gate 1.3: Performance Baseline Established**
**Expected Results:**
- ‚ùå WASM performance not measurable (WASM not functional)
- ‚úÖ Local OpenSCAD baseline established
- ‚úÖ Memory profiling infrastructure working
- ‚úÖ 2GB constraint validation available

**Validation Command:**
```bash
# Should show performance gaps but establish baselines
uv run pytest tests/test_performance_baseline.py -v -s
```

---

## üö® **Risk Mitigation & Contingency Plans**

### **High-Risk Areas**
1. **Playwright Setup Complexity** - Browser deps, CI environment
2. **Marimo Programmatic Execution** - API limitations, timeout issues  
3. **Browser Test Stability** - Flaky tests, timing issues

### **Mitigation Strategies**

#### **Playwright Stability**
```bash
# Retry mechanism for flaky browser tests
uv run pytest tests/test_e2e_*.py --reruns 2 --reruns-delay 1
```

#### **Marimo Execution Fallback**
```python
# If programmatic marimo fails, use subprocess with timeout
try:
    # Direct marimo API approach
    result = marimo.run_notebook(notebook_path)
except ImportError:
    # Fallback to subprocess
    result = subprocess.run(['marimo', 'run', notebook_path], ...)
```

#### **CI Environment Docker**
```dockerfile
# Dockerfile.testing - For consistent CI environment
FROM mcr.microsoft.com/playwright:v1.40.0-focal
RUN pip install uv
COPY . /app
WORKDIR /app
RUN uv sync --group dev
CMD ["uv", "run", "pytest", "tests/test_e2e_*.py"]
```

### **Contingency Plans**
- **Playwright fails:** Fallback to Selenium WebDriver
- **Marimo execution fails:** Mock notebook state, document limitation
- **Performance tests flaky:** Focus on functional problem detection

---

## üìä **Success Metrics & KPIs**

### **Phase 1 Success Criteria**
- ‚úÖ **0% False Positive Confidence** - No mocks hiding real problems
- ‚úÖ **100% Real Problem Visibility** - All 4 critical issues documented
- ‚úÖ **Infrastructure Ready** - Foundation for Phase 2-4 validation
- ‚úÖ **Honest Test Results** - Test failures accurately reflect system state

### **Quantified Targets**
- **Browser Test Coverage:** >5 critical anywidget scenarios
- **Marimo Test Coverage:** >3 variable conflict scenarios  
- **Performance Baselines:** Local OpenSCAD established, WASM gaps documented
- **CI Integration:** All tests run in GitHub Actions (expected failures OK)

### **Documentation Outputs**
- **Problem Report:** Detailed documentation of each critical issue
- **Test Infrastructure:** Reusable E2E testing framework
- **Performance Baseline:** Honest performance measurements
- **Fix Validation Foundation:** Ready for Phase 2 implementation

---

## üìÖ **Implementation Timeline**

### **Day 1: Infrastructure Setup**
- **Morning:** Playwright installation & configuration (Step 1.1.1)
- **Afternoon:** anywidget browser test creation (Step 1.1.2)

### **Day 2: Marimo Integration** 
- **Morning:** Real Marimo execution tests (Step 1.1.3)
- **Afternoon:** CI integration setup (Step 1.1.4)

### **Day 3: Critical Paths**
- **Full Day:** User journey testing (Step 1.2)

### **Day 4: Performance & Validation**
- **Morning:** Performance baseline tests (Step 1.3)
- **Afternoon:** Final validation & documentation

---

**Phase Status:** ‚úÖ 80% IMPLEMENTED - Ready for Test Execution  
**Next Action:** Execute existing tests: `uv run pytest tests/test_e2e_*.py -v`  
**Missing:** Marimo notebook tests, Performance baselines  
**Expected Completion:** 17. Juni 2025

**‚ö†Ô∏è Important:** Test failures are EXPECTED and DESIRED in Phase 1 - they document real problems!