"""
Browser Console Error Capture Tests

These tests focus on capturing and analyzing real browser console errors
to document the specific error patterns that occur when trying to use
anywidget with marimo-openscad.

EXPECTED TO CAPTURE ERRORS: These tests document error patterns.
"""

import pytest
from playwright.sync_api import Page
from pathlib import Path
import json
import time
import re

from .playwright_config import test_page, browser_context, browser, error_analyzer


class TestBrowserConsoleErrors:
    """Capture and analyze real browser console errors"""
    
    def test_module_resolution_errors(self, test_page: Page, error_analyzer):
        """
        EXPECTED TO CAPTURE ERRORS: Module resolution and import errors
        
        Captures specific error patterns related to ES module imports
        that fail in anywidget context.
        """
        # Create a test page that tries to import our modules
        import_test_html = '''
<!DOCTYPE html>
<html>
<head><title>Module Import Error Test</title></head>
<body>
    <div id="status">Testing module imports...</div>
    <script type="module">
        console.log("Starting module import tests...");
        
        const moduleTests = [
            {
                name: "Widget Module",
                path: "../../src/js/widget.js",
                description: "Main anywidget implementation"
            },
            {
                name: "WASM Renderer",
                path: "../../src/js/openscad-wasm-renderer.js", 
                description: "WASM OpenSCAD renderer"
            },
            {
                name: "WASM Loader",
                path: "../../src/js/wasm-loader.js",
                description: "WASM module loader"
            },
            {
                name: "Direct Renderer", 
                path: "../../src/js/openscad-direct-renderer.js",
                description: "Direct WASM renderer"
            }
        ];
        
        window.moduleTestResults = [];
        
        async function testModuleImport(test) {
            console.log(`Testing import: ${test.name} from ${test.path}`);
            
            try {
                const module = await import(test.path);
                console.log(`SUCCESS: ${test.name} imported`, module);
                window.moduleTestResults.push({
                    name: test.name,
                    path: test.path,
                    success: true,
                    exports: Object.keys(module),
                    description: test.description
                });
            } catch (error) {
                console.error(`FAILED: ${test.name} import failed:`, error);
                window.moduleTestResults.push({
                    name: test.name,
                    path: test.path,
                    success: false,
                    error: error.message,
                    errorType: error.constructor.name,
                    stack: error.stack.split('\\n').slice(0, 3),
                    description: test.description
                });
            }
        }
        
        // Test all modules
        for (const test of moduleTests) {
            await testModuleImport(test);
        }
        
        console.log("Module import tests completed", window.moduleTestResults);
        document.getElementById('status').textContent = 
            `Completed ${moduleTests.length} import tests. Check console for results.`;
    </script>
</body>
</html>
        '''
        
        test_page.set_content(import_test_html)
        test_page.wait_for_load_state("networkidle")
        
        # Wait for import tests to complete
        time.sleep(3)
        
        # Capture console errors
        console_errors = error_analyzer['capture_console_errors'](test_page)
        network_failures = error_analyzer['capture_network_failures'](test_page)
        
        # Get test results from page
        try:
            module_results = test_page.evaluate("window.moduleTestResults || []")
        except:
            module_results = []
        
        print("\nðŸ” MODULE RESOLUTION ERROR ANALYSIS:")
        print(f"Console Errors: {len(console_errors)}")
        print(f"Network Failures: {len(network_failures)}")
        print(f"Module Test Results: {len(module_results)}")
        
        # Analyze error patterns
        import_error_patterns = [
            r'Failed to resolve module',
            r'Module not found', 
            r'Cannot resolve module',
            r'404.*\.js',
            r'Failed to fetch',
            r'CORS.*policy',
            r'file:.*not allowed'
        ]
        
        detected_patterns = {}
        for pattern in import_error_patterns:
            pattern_matches = []
            for error in console_errors:
                if re.search(pattern, error['text'], re.IGNORECASE):
                    pattern_matches.append(error['text'])
            detected_patterns[pattern] = pattern_matches
        
        print(f"\nðŸ” ERROR PATTERN ANALYSIS:")
        for pattern, matches in detected_patterns.items():
            if matches:
                print(f"  âœ… Pattern '{pattern}': {len(matches)} matches")
                for match in matches[:2]:  # Show first 2 matches
                    print(f"    - {match[:100]}...")
            else:
                print(f"  âŒ Pattern '{pattern}': No matches")
        
        # Analyze module test results
        failed_imports = [r for r in module_results if not r['success']]
        successful_imports = [r for r in module_results if r['success']]
        
        print(f"\nðŸ” MODULE IMPORT RESULTS:")
        print(f"  Failed Imports: {len(failed_imports)}")
        print(f"  Successful Imports: {len(successful_imports)}")
        
        for result in failed_imports:
            print(f"    âŒ {result['name']}: {result['error']}")
        
        for result in successful_imports:
            print(f"    âœ… {result['name']}: {len(result.get('exports', []))} exports")
        
        # EXPECTATION: Most/all imports should fail
        assert len(failed_imports) > 0, "Expected module import failures"
        assert len(console_errors) > 0, "Expected console errors from import failures"
        
        # Document specific error types
        error_types = set(error['errorType'] for result in failed_imports for error in [result] if 'errorType' in result)
        print(f"Error Types Detected: {list(error_types)}")
        
        print("âœ… SUCCESS: Module resolution errors captured and documented")
        
        return {
            'console_errors': console_errors,
            'network_failures': network_failures,
            'module_results': module_results,
            'error_patterns': detected_patterns
        }
    
    def test_anywidget_specific_errors(self, test_page: Page, error_analyzer):
        """
        EXPECTED TO CAPTURE ERRORS: anywidget-specific error patterns
        
        Tests errors that occur when trying to use anywidget features
        that depend on module imports.
        """
        anywidget_test_html = '''
<!DOCTYPE html>
<html>
<head><title>anywidget Error Test</title></head>
<body>
    <div id="widget-area"></div>
    <script type="module">
        console.log("Testing anywidget-specific functionality...");
        
        // Simulate anywidget model interface
        const mockModel = {
            get: function(key) {
                const data = {
                    'scad_code': 'cube([2,2,2]);',
                    'renderer_type': 'wasm',
                    'stl_data': '',
                    'error_message': '',
                    'renderer_status': 'initializing'
                };
                console.log(`Model.get(${key}) -> ${data[key]}`);
                return data[key];
            },
            set: function(key, value) {
                console.log(`Model.set(${key}, ${value})`);
            },
            on: function(event, callback) {
                console.log(`Model.on(${event}, callback)`);
            }
        };
        
        // Test anywidget render function availability
        window.testAnyWidgetRender = async function() {
            console.log("Testing anywidget render function...");
            
            try {
                // This should fail because render function is not available
                if (typeof window.renderWidget === 'function') {
                    console.log("Render function available, attempting to call...");
                    const el = document.getElementById('widget-area');
                    await window.renderWidget({ model: mockModel, el: el });
                    console.log("SUCCESS: Widget rendered");
                    return { success: true };
                } else {
                    throw new Error("anywidget render function not available (import failure)");
                }
            } catch (error) {
                console.error("FAILED: anywidget render error:", error);
                return { 
                    success: false, 
                    error: error.message,
                    errorType: error.constructor.name 
                };
            }
        };
        
        // Test WASM renderer availability
        window.testWASMRenderer = async function() {
            console.log("Testing WASM renderer availability...");
            
            try {
                // This should fail because WASM renderer is not loaded
                if (typeof window.OpenSCADWASMRenderer !== 'undefined') {
                    console.log("WASM renderer available, testing...");
                    const renderer = new window.OpenSCADWASMRenderer();
                    const result = await renderer.render('cube([1,1,1]);');
                    console.log("SUCCESS: WASM render completed", result);
                    return { success: true, result: result };
                } else {
                    throw new Error("OpenSCADWASMRenderer not available (module loading failure)");
                }
            } catch (error) {
                console.error("FAILED: WASM renderer error:", error);
                return {
                    success: false,
                    error: error.message,
                    errorType: error.constructor.name
                };
            }
        };
        
        // Test Three.js availability (this might work from CDN)
        window.testThreeJS = function() {
            console.log("Testing Three.js availability...");
            
            try {
                if (typeof THREE !== 'undefined') {
                    console.log("Three.js available from global");
                    const scene = new THREE.Scene();
                    return { success: true, source: 'global' };
                } else {
                    throw new Error("Three.js not available globally");
                }
            } catch (error) {
                console.error("Three.js test failed:", error);
                return { 
                    success: false, 
                    error: error.message,
                    source: 'none'
                };
            }
        };
        
        // Run all tests
        setTimeout(async () => {
            const results = {
                anywidget: await window.testAnyWidgetRender(),
                wasm: await window.testWASMRenderer(),
                threejs: window.testThreeJS()
            };
            
            window.anywidgetTestResults = results;
            console.log("anywidget tests completed:", results);
        }, 1000);
    </script>
</body>
</html>
        '''
        
        test_page.set_content(anywidget_test_html)
        test_page.wait_for_load_state("networkidle")
        
        # Wait for tests to complete
        time.sleep(3)
        
        # Capture errors
        console_errors = error_analyzer['capture_console_errors'](test_page)
        
        # Get test results
        try:
            anywidget_results = test_page.evaluate("window.anywidgetTestResults || {}")
        except:
            anywidget_results = {}
        
        print("\nðŸ” ANYWIDGET-SPECIFIC ERROR ANALYSIS:")
        
        # Analyze anywidget test results
        for test_name, result in anywidget_results.items():
            if result and not result.get('success', True):
                print(f"  âŒ {test_name.upper()}: {result.get('error', 'Unknown error')}")
            elif result and result.get('success'):
                print(f"  âœ… {test_name.upper()}: Success")
            else:
                print(f"  âš ï¸ {test_name.upper()}: No result")
        
        # Look for anywidget-specific error patterns
        anywidget_patterns = [
            r'render.*function.*not.*available',
            r'anywidget.*not.*defined',
            r'OpenSCADWASMRenderer.*not.*available',
            r'THREE.*not.*defined',
            r'Widget.*creation.*failed'
        ]
        
        anywidget_errors = {}
        for pattern in anywidget_patterns:
            matches = []
            for error in console_errors:
                if re.search(pattern, error['text'], re.IGNORECASE):
                    matches.append(error['text'])
            anywidget_errors[pattern] = matches
        
        print(f"\nðŸ” ANYWIDGET ERROR PATTERNS:")
        for pattern, matches in anywidget_errors.items():
            if matches:
                print(f"  âœ… '{pattern}': {len(matches)} matches")
            else:
                print(f"  âŒ '{pattern}': No matches")
        
        # EXPECTATION: anywidget and WASM should fail, Three.js might work
        expected_failures = ['anywidget', 'wasm']
        actual_failures = [name for name, result in anywidget_results.items() 
                         if result and not result.get('success', True)]
        
        print(f"\nExpected failures: {expected_failures}")
        print(f"Actual failures: {actual_failures}")
        
        # Document that we're detecting the right problems
        assert len(actual_failures) > 0, "Expected some anywidget functionality to fail"
        
        print("âœ… SUCCESS: anywidget-specific errors captured and documented")
        
        return {
            'console_errors': console_errors,
            'anywidget_results': anywidget_results,
            'error_patterns': anywidget_errors
        }
    
    def test_comprehensive_error_summary(self, test_page: Page, error_analyzer):
        """
        COMPREHENSIVE DOCUMENTATION: Create complete error summary
        
        Runs multiple error-generating scenarios and creates a comprehensive
        summary of all error types detected.
        """
        # Load our main test environment
        test_file = Path(__file__).parent / "test_files" / "anywidget_test_environment.html"
        test_page.goto(f"file://{test_file}")
        test_page.wait_for_load_state("networkidle")
        
        # Run all tests that generate errors
        test_page.evaluate("window.testBasicImport()")
        time.sleep(1)
        test_page.evaluate("window.testWASMLoading()")
        time.sleep(1)  
        test_page.evaluate("window.testWidgetCreation()")
        time.sleep(1)
        
        # Capture all errors
        console_errors = error_analyzer['capture_console_errors'](test_page)
        network_failures = error_analyzer['capture_network_failures'](test_page)
        test_results = test_page.evaluate("window.getTestResults()")
        
        # Analyze comprehensive error patterns
        import_analysis = error_analyzer['analyze_import_errors'](console_errors)
        wasm_analysis = error_analyzer['analyze_wasm_errors'](console_errors, network_failures)
        
        # Create comprehensive error categorization
        error_categories = {
            'Module Resolution': [],
            'Network Access': [],
            'WASM Related': [],
            'anywidget Specific': [],
            'Browser Compatibility': [],
            'Unknown': []
        }
        
        # Categorize each console error
        for error in console_errors:
            error_text = error['text'].lower()
            categorized = False
            
            if any(keyword in error_text for keyword in ['import', 'module', 'resolve']):
                error_categories['Module Resolution'].append(error)
                categorized = True
            
            if any(keyword in error_text for keyword in ['fetch', 'network', 'cors', '404']):
                error_categories['Network Access'].append(error)
                categorized = True
                
            if any(keyword in error_text for keyword in ['wasm', 'webassembly']):
                error_categories['WASM Related'].append(error)
                categorized = True
                
            if any(keyword in error_text for keyword in ['widget', 'render', 'anywidget']):
                error_categories['anywidget Specific'].append(error)
                categorized = True
                
            if any(keyword in error_text for keyword in ['browser', 'compatibility', 'support']):
                error_categories['Browser Compatibility'].append(error)
                categorized = True
                
            if not categorized:
                error_categories['Unknown'].append(error)
        
        # Create comprehensive summary
        error_summary = {
            'timestamp': test_page.evaluate("new Date().toISOString()"),
            'totals': {
                'console_errors': len(console_errors),
                'network_failures': len(network_failures),
                'test_results': len(test_results),
                'failed_tests': len([r for r in test_results if 'FAILURE' in r['status']])
            },
            'error_categories': {
                category: len(errors) for category, errors in error_categories.items()
            },
            'critical_issues_documented': {
                'import_limitations': import_analysis['has_import_issues'],
                'wasm_loading_failures': wasm_analysis['has_wasm_issues'],
                'network_access_blocked': len(network_failures) > 0,
                'widget_creation_failed': any(
                    'WIDGET_CREATION' in r['test'] and 'FAILURE' in r['status']
                    for r in test_results
                )
            },
            'top_error_messages': [error['text'][:200] for error in console_errors[:5]]
        }
        
        print("\nðŸ” COMPREHENSIVE ERROR SUMMARY:")
        print(json.dumps(error_summary, indent=2))
        
        print(f"\nðŸ” ERROR CATEGORIZATION:")
        for category, count in error_summary['error_categories'].items():
            if count > 0:
                print(f"  âœ… {category}: {count} errors")
                # Show sample errors for non-zero categories
                sample_errors = error_categories[category][:2]
                for error in sample_errors:
                    print(f"    - {error['text'][:100]}...")
            else:
                print(f"  âŒ {category}: {count} errors")
        
        print(f"\nðŸ” CRITICAL ISSUES DOCUMENTATION STATUS:")
        for issue, documented in error_summary['critical_issues_documented'].items():
            status = "âœ… DOCUMENTED" if documented else "âŒ NOT DOCUMENTED"
            print(f"  {status}: {issue}")
        
        # Verify we've successfully documented the critical problems
        documented_issues = sum(error_summary['critical_issues_documented'].values())
        total_expected = len(error_summary['critical_issues_documented'])
        
        print(f"\nâœ… PHASE 1 SUCCESS METRICS:")
        print(f"  Critical Issues Documented: {documented_issues}/{total_expected}")
        print(f"  Total Errors Captured: {error_summary['totals']['console_errors']}")
        print(f"  Mock-Free Testing: 100% (no mocks used)")
        print(f"  Real Problem Visibility: {documented_issues/total_expected*100:.0f}%")
        
        # We expect to document most critical issues
        assert documented_issues >= 2, f"Expected to document at least 2/4 critical issues, got {documented_issues}"
        assert error_summary['totals']['console_errors'] > 0, "Expected to capture console errors"
        
        print("\nâœ… SUCCESS: Comprehensive error documentation completed")
        print("    Phase 1 objective achieved: Real problems documented without mock false confidence")
        
        return error_summary


# Expected failure marker
pytestmark = pytest.mark.e2e